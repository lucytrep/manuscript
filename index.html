<!doctype html>
<html>
	<head>
		<title>Lucy's Manuscript</title>
		<link href="style.css" rel="stylesheet">
		<link href="reset.css" rel="stylesheet">
	</head>
	<body>
		<header>
			<h1>PROJECT <span class="wild-font">N º1</span></h1>
			<nav>
				<!-- <1-- T0D0 implement nav-->
			</nav>
		</header>
		
		<main>
	
			<h2 class="bigpink">WHAT IS <span class="special-font"> CODE?</span></h2>

				<p class="left-align">June 11, 2015 <p class="right-align">By Paul Ford</p></p>

			<h3><code class="code">&lt;em&gt;</code> Let’s Begin <code class="code">&lt;em&gt;</code></h3>
				<p>A computer is a clock with benefits. They all work the same, doing second-grade math, one step at a time: Tick, take a number and put it in box one. Tick, take another number, put it in box two. Tick, operate (an operation might be addition or subtraction) on those two numbers and put the resulting number in box one. Tick, check if the result is zero, and if it is, go to some other box and follow a new set of instructions.</p>

				<p>You, using a pen and paper, can do anything a computer can; you just can’t do those things billions of times per second. And those billions of tiny operations add up. They can cause a phone to boop, elevate an elevator, or redirect a missile. That raw speed makes it possible to pull off not one but multiple sleights of hand, card tricks on top of card tricks. Take a bunch of pulses of light reflected from an optical disc, apply some math to unsqueeze them, and copy the resulting pile of expanded impulses into some memory cells—then read from those cells to paint light on the screen. Millions of pulses, 60 times a second. That’s how you make the rubes believe they’re watching a movie.</p>

				<p>Apple has always made computers; Microsoft used to make only software (and occasional accessory hardware, such as mice and keyboards), but now it’s in the hardware business, with Xbox game consoles, Surface tablets, and Lumia phones. Facebook assembles its own computers for its massive data centers.
				So many things are computers, or will be. That includes watches, cameras, air conditioners, cash registers, toilets, toys, airplanes, and movie projectors. Samsung makes computers that look like TVs, and Tesla makes computers with wheels and engines. Some things that aren’t yet computers—dental floss, flashlights—will fall eventually.</p>

				<p>When you “batch” process a thousand images in Photoshop or sum numbers in Excel, you’re programming, at least a little. When you use computers too much—which is to say a typical amount—they start to change you. I’ve had Photoshop dreams, Visio dreams, spreadsheet dreams, and Web browser dreams. The dreamscape becomes fluid and can be sorted and restructured. I’ve had programming dreams where I move text around the screen.
				You can make computers do wonderful things, but you need to understand their limits. They’re not all-powerful, not conscious in the least. They’re fast, but some parts—the processor, the RAM—are faster than others—like the hard drive or the network connection. Making them seem infinite takes a great deal of work from a lot of programmers and a lot of marketers.</p>

				<p>The turn-of-last-century British artist William Morris once said you can’t have art without resistance in the materials. The computer and its multifarious peripherals are the materials. The code is the art.</p>

			<h3><code class="code">&lt;em&gt;</code> How Do You Type an “A”? <code class="code">&lt;em&gt;</code></h3>

				<p>Consider what happens when you strike a key on your keyboard. Say a lowercase “a.” The keyboard is waiting for you to press a key, or release one; it’s constantly scanning to see what keys are pressed down. Hitting the key sends a scancode.</p>
				<p>keyboard is waiting for a key to be pressed, the computer is waiting for a signal from the keyboard. When one comes down the pike, the computer interprets it and passes it farther into its own interior. “Here’s what the keyboard just received—do with this what you will.”</p>
				<p>It’s simple now, right? The computer just goes to some table, figures out that the signal corresponds to the letter “a,” and puts it on screen. Of course not—too easy. Computers are machines. They don’t know what a screen or an “a” are. To put the “a” on the screen, your computer has to pull the image of the “a” out of its memory as part of a font, an “a” made up of lines and circles. It has to take these lines and circles and render them in a little box of pixels in the part of its memory that manages the screen. So far we have at least three representations of one letter: the signal from the keyboard; the version in memory; and the lines-and-circles version sketched on the screen. We haven’t even considered how to store it, or what happens to the letters to the left and the right when you insert an “a” in the middle of a sentence. Or what “lines and circles” mean when reduced to binary data. There are surprisingly many ways to represent a simple “a.” It’s amazing any of it works at all.</p>
				<p>Coders are people who are willing to work backward to that key press. It takes a certain temperament to page through standards documents, manuals, and documentation and read things like “data fields are transmitted least significant bit first” in the interest of understanding why, when you expected “ü,” you keep getting “�.”</p>

			<h3><code class="code">&lt;em&gt;</code> How Does Code Become Software? <code class="code">&lt;em&gt;</code></h3>

				<p>We know that a computer is a clock with benefits, and that software starts as code, but how?</p>
					
				<p>We know that someone, somehow, enters a program into the computer and the program is made of code. In the old days, that meant putting holes in punch cards. Then you’d put the cards into a box and give them to an operator who would load them, and the computer would flip through the cards, identify where the holes were, and update parts of its memory, and then it would—OK, that’s a little too far back. Let’s talk about modern typing-into-a-keyboard code. It might look like this:</p>

					<p><code>ispal: {~x|x}</code></p>

				<p>That code will test if something is a palindrome. If you next typed in ispal "able was i ere i saw elba", K will confirm that yes, this is a palindrome.</p>

				<p>So how else might your code look? Maybe like so, in Excel (with all the formulas hidden away under the numbers they produce, and a check box that you can check).</p>

					<p><code>&lt;img&gt;</code></p>

				<p>But Excel spreadsheets are tricky, because they can hide all kinds of things under their numbers. This opacity causes risks. One study by a researcher at the University of Hawaii found that 88 percent of spreadsheets contain errors.</p>

				<p>Programming can also look like Scratch, a language for kids.</p>

					<p><code>&lt;img&gt;</code></p>

				<p>That’s definitely programming right there—the computer is waiting for a click, for some input, just as it waits for you to type an “a,” and then it’s doing something repetitive, and it involves hilarious animals.</p>

					<p>Or maybe:</p>

					<p><code>PRINT*,"WHY WON'T WORK<br>END</code></p>

				<p>That’s in Fortran. The reason it’s not working is that you forgot to put a quotation mark at the end of the first line. Try a little harder, thanks.</p>

				<p>All of these things are coding of one kind or another, but the last bit is what most programmers would readily identify as code. A sequence of symbols (using typical keyboard characters, saved to a file of some kind) that someone typed in, or copied, or pasted from elsewhere. That doesn’t mean the other kinds of coding aren’t valid or won’t help you achieve your goals. Coding is a broad human activity, like sport, or writing. When software developers think of coding, most of them are thinking about lines of code in files. They’re handed a problem, think about the problem, write code that will solve the problem, and then expect the computer to turn word into deed.</p>

				<p>Code is inert. How do you make it ert? You run software that transforms it into machine language. The word “language” is a little ambitious here, given that you can make a computing device with wood and marbles. Your goal is to turn your code into an explicit list of instructions that can be carried out by interconnected logic gates, thus turning your code into something that can be executed—software.</p>

				<p>A compiler is software that takes the symbols you typed into a file and transforms them into lower-level instructions. Imagine a programming language called Business Operating Language United System, or Bolus. It’s a terrible language that will have to suffice for a few awkward paragraphs. It has one real command, PRINT. We want it to print HELLO NERDS on our screen. To that end, we write a line of code in a text file that says:</p>

					<p><code>PRINT {HELLO NERDS}</code></p>

				<p>And we save that as nerds.bol. Now we run gnubolus nerds.bol, our imaginary compiler program. How does it start? The only way it can: by doing lexical analysis, going character by character, starting with the “p,” grouping characters into tokens, saving them into our one-dimensional tree boxes.</p>

					<p><code>&lt;img&gt;</code></p>

				<p>The reason I’m showing it to you is so you can see how every character matters. Computers usually “understand” things by going character by character, bit by bit, transforming the code into other kinds of code as they go. The Bolus compiler now organizes the tokens into a little tree. Kind of like a sentence diagram. Except instead of nouns, verbs, and adjectives, the computer is looking for functions and arguments.</p>

					<p><code>&lt;img&gt;</code></p>

				<p>Trees are a really pleasant way of thinking of the world. Your memo at work has sections that have paragraphs? Tree. Your e-mail program contains messages that contain subject lines and addresses? Tree. Your favorite software program that has a menu bar with individual items that have subitems? Tree. Every day is Arbor Day in Codeville.</p>

				<p>Of course, it’s all a trick. If you cut open a computer, you’ll find countless little boxes in rows, places where you can put and retrieve bytes. Everything ultimately has to get down to things in little boxes pointing to each other. That’s just how things work.</p>

					<p><code>&lt;img&gt;</code></p>

				<p>Every character truly, truly matters. Every single stupid misplaced semicolon, space where you meant tab, bracket instead of a parenthesis—mistakes can leave the computer in a state of panic. The trees don’t know where to put their leaves. Their roots decay. The boxes don’t stack neatly. For not only are computers as dumb as a billion marbles, they’re also positively Stradivarian in their delicacy.</p>

				<p>That process of going character by character can be wrapped up into a routine—also called a function, a method, a subroutine, or component. (Little in computing has a single, reliable name, which means everyone is always arguing over semantics.) And that routine can be run as often as you need. Second, you can print anything you wish, not just one phrase. Third, you can repeat the process forever, and nothing will stop you until the machine breaks or, barring that, heat death of the universe. Obviously no one besides Jack Nicholson in The Shining really needs to keep typing the same phrase over and over, and even then it turned out to be a bad idea.</p>

				<p>Instead of worrying about where the words are stored in memory and having to go character by character, programming languages let you think of things like strings, arrays, and trees. That’s what programming gives you. You may look over a programmer’s shoulder and think the code looks complex and boring, but it’s covering up repetitive boredom that’s unimaginably vast.</p>

				<p>This thing we just did with individual characters, compiling a program down into a fake assembly language so that the nonexistent computer can print each character one at a time? The same principle applies to every pixel on your screen, every frequency encoded in your MP3 files, and every imaginary cube in Minecraft. Computing treats human language as an arbitrary set of symbols in sequences. It treats music, imagery, and film that way, too.</p>

				<p>It’s a good and healthy exercise to ponder what your computer is doing right now. Maybe you’re reading this on a laptop: What are the steps and layers between what you’re doing and the Lilliputian mechanisms within? When you double-click an icon to open a program such as a word processor, the computer must know where that program is on the disk. It has some sort of accounting process to do that. And then it loads that program into its memory—which means that it loads an enormous to-do list into its memory and starts to step through it. What does that list look like?</p>

				<p>Maybe you’re reading this in print. No shame in that. In fact, thank you. The paper is the artifact of digital processes. Remember how we put that “a” on screen? See if you can get from some sleepy writer typing that letter on a keyboard in Brooklyn, N.Y., to the paper under your thumb. What framed that fearful symmetry?</p>

				<p>Thinking this way will teach you two things about computers: One, there’s no magic, no matter how much it looks like there is. There’s just work to make things look like magic. And two, it’s crazy in there.</p>

					<p><code>&lt;img&gt;</code></p>

			<h3><code class="code">&lt;em&gt;</code> What’s With All These Conferences, Anyway? <code class="code">&lt;em&gt;</code></h3>

				<p>Conferences! The website Lanyrd lists hundreds of technology conferences for June 2015. There’s an event for software testers in Chicago, a Twitter conference in São Paulo, and one on enterprise content management in Amsterdam. In New York alone there’s the Big Apple Scrum Day, the Razorfish Tech Summit, an entrepreneurship boot camp for veterans, a conference dedicated to digital mapping, many conferences for digital marketers, one dedicated to Node.js, one for Ruby, and one for Scala (these are programming languages), a couple of breakfasts, a conference for cascading style sheets, one for text analytics, and something called the Employee Engagement Awards.</p>

				<p>Tech conferences look like you’d expect. Tons of people at a Sheraton, keynote in Ballroom D. Or enormous streams of people wandering through South by Southwest in Austin. People come together in the dozens or thousands and attend panels, ostensibly to learn; they attend presentations and brush up their skills, but there’s a secondary conference function, one of acculturation. You go to a technology conference to affirm your tribal identity, to transfer out of the throng of dilettantes and into the zone of the professional. You pick up swag and talk to vendors, if that’s your thing.</p>

					<p><code>&lt;img&gt;</code></p>

				<p>Technology conferences are where primate dynamics can be fully displayed, where relationships of power and hierarchy can be established. There are keynote speakers—often the people who created the technology at hand or crafted a given language. There are the regular speakers, often paid not at all or in airfare, who present some idea or technique or approach. Then there are the panels, where a group of people are lined up in a row and forced into some semblance of interaction while the audience checks its e-mail.</p>

				p>I’m a little down on panels. They tend to drift. I’m not sure why they exist.</p>

				<p>Here’s the other thing about technology conferences: There has been much sexual harassment and much sexist content in conferences. Which is stupid, because computers are dumb rocks lacking genitalia, but there you have it.</p>

				<p>Women in software, having had enough, started to write it up, post to blogs. Other women did the same. The problem is pervasive: There are a lot of conferences, and there have been many reports of harassing behavior. The language Ruby, the preferred language for startup bros, developed the worst reputation. At a Ruby conference in 2009, someone gave a talk subtitled “Perform Like a Pr0n Star,” with sexy slides. That was dispiriting. There have been criminal incidents, too.</p>

				<p>Conferences began to develop codes of conduct, rules and algorithms for people (men, really) to follow.</p>

					<blockquote> 
					If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible …<br>
					— Burlington Ruby Conference
					</blockquote>

					<blockquote> 
					php[architect] is dedicated to providing a harassment-free event experience for everyone and will not tolerate harassment or offensive behavior in any form.<br>
					— php[architect]
					</blockquote>

					<blockquote> 
					The Atlanta Java Users Group (AJUG) is dedicated to providing an outstanding conference experience for all attendees, speakers, sponsors, volunteers, and organizers involved in DevNexus (GeekyNerds) regardless of gender, sexual orientation, disability, physical appearance, body size, race, religion, financial status, hair color (or hair amount), platform preference, or text editor of choice.<br>
					— devnexus
					</blockquote>

				<p>When people started talking about conference behavior, they also began to talk about the larger problems of programming culture. This was always an issue, but the conference issues gave people a point of common reference. Why were there so many men in this field? Why do they behave so strangely? Why is it so hard for them to be in groups with female programmers and behave in a typical, adult way?</p>

					<p><code>&lt;img&gt;</code></p>

				<p>“I go to work and I stick out like a sore thumb. I have been mistaken for an administrative assistant more than once. I have been asked if I was physical security (despite security wearing very distinctive uniforms),” wrote Erica Joy Baker on Medium.com who has worked, among other places, at Google.Δ</p>

			<h4>Famous women in coding history</h4>

				<p>Ada Lovelace: The first programmer. She devised algorithms for Charles Babbage’s “analytical engine,” which he never built.</p>

					<p><code>&lt;img&gt;</code></p>

				<p>Grace Murray Hopper: World War II hero and inventor of the compiler.</p>

					<p><code>&lt;img&gt;</code></p>

				<p>“Always the only woman in the meeting, often the first—the first female R&D engineer, first female project lead, first female software team lead—in the companies I worked for,” wrote another woman in Fast Company magazine.</p>

				<p>Fewer than a fifth of undergraduate degrees in computer science awarded in 2012 went to women, according to the National Center for Women & Information Technology. Less than 30 percent of the people in computing are women. And the number of women in computing has fallen since the 1980s, even as the market for their skills has expanded. The pipeline is a huge problem. And yet it’s not unsolvable. I’ve met managers who have built perfectly functional large teams that are more than half female coders. Places such as the handicrafts e-commerce site Etsy have made a particular effort to develop educational programs and mentorship programs. Organizations such as the not-for-profit Girl Develop It teach women, and just women, how to create software.</p>

				<p>It’s all happening very late in the boom, though. In 2014 some companies began to release diversity reports for their programming teams. It wasn’t a popular practice, but it was revealing. Intel is 23 percent female; Yahoo! is 37 percent. Apple, Facebook, Google, Twitter, and Microsoft are all around 30 percent. These numbers are for the whole companies, not only programmers. That’s a lot of women who didn’t get stock options. The numbers of people who aren’t white or Asian are worse yet. Apple just gave $50 million to fund diversity initiatives, equivalent to 0.007 percent of its market cap. Intel has a $300 million diversity project.</p>

				<p>The average programmer is moderately diligent, capable of basic mathematics, has a working knowledge of one or more programming languages, and can communicate what he or she is doing to management and his or her peers. Given that a significant number of women work as journalists and editors, perform surgery, run companies, manage small businesses, and use spreadsheets, that a few even serve on the Supreme Court, and that we are no longer surprised to find women working as accountants, professors, statisticians, or project managers, it’s hard to imagine that they can’t write JavaScript. Programming, despite the hype and the self-serving fantasies of programmers the world over, isn’t the most intellectually demanding task imaginable.</p>

				<p>Which leads one to the inescapable conclusion: The problem with women in technology isn’t the women.</p>

					<p><code>&lt;img&gt;</code></p>
			
			<h2 class="green-background">REFLECTION</h2>
			
					<p class="left-align">September 11, 2025<p class="right-align">By Lucy Trepanier</p></p>
				
				<p>As a graphic and digital designer, I can appreciate the parallel between code and design. Both are incredibly meticulous, and each step contributes to the final product. With coding, you are problem-solving through each letter, word, <code>&lt;div&gt;</code>, etc., and creating an explicit list of instructions carried out by logic, which is then turned into software (Ford, 2015). In my experience, design works similarly. The wrong choice of imagery, colour, or font can jar a user just as much as seeing raw HTML code on a website. As a designer, I am curious to see how much I can relate and apply my previous knowledge, perfectionism, and personal differences to this subject. Art directors questioning my designs and developers explaining issues with my Figma frames have shown me that design and code can be inextricably linked. 
			
			Reading What Is Code? made me reflect on why it’s taken me so long to finally learn and explore the world of interaction design. I have wanted to do this for years, but why was I so hesitant before? Ford brings up things in his discussion that stood out to me, and why internal or external factors have affected me thus far.
			
			It is important to realize that coding is similar to learning a new world language. However, unlike spoken language taught through interaction, coding is strictly on screen and with the computer. Mistakes offer no forgiveness, and the code either works or it does not, with little to no explanation. This makes it daunting, frustrating, and so much more… especially for us beginners.
			
			Ford’s discussion highlights how coding is largely a male-dominated environment that has intimidated many women from pursuing it. But we see historically, women have been among the true pioneers! From my previous experience as a graphic designer, I’ve seen how gender diversity and different perspectives add value and strengthen a final product. When design and code backgrounds work together and intertwine, something beautiful can be created. I am optimistic that more women and diverse groups will become involved as technology expands. Like design, code has so much more creative potential, and we must put the stereotypes aside. This makes me question why Ford describes it as “complex and boring” (Ford, 2015). I think coding holds a lot of possibilities for playfulness and innovation, particularly when paired with curiosity and design thinking.
			
				Coding can be intimidating, but it calls for a broader audience. As Ford infers, the issue lies in the external environment, so it will be up to those within it to push for a more inclusive and welcoming community. Diversity and participation from all backgrounds can only heighten coding's impact. I am encouraged and excited that our class is predominantly female from all over the world. This gives me so much hope for women in STEM and that the industry is shifting. Women are some of the most revolutionary parts of code’s timeline, including Ada Lovelace and Grace Murray (Ford, 2015). Diverse voices spark new and innovative ideas that break down what we currently know. Despite the challenges I personally face as a neurodivergent learner, I see a window of opportunity, and I’m ready to participate and make an impact.</p>
			</details>
		</main>

		<footer>
			<small>(c) 2025 - Lucy Trepanier</small>
		</footer>
	</body>
</html>
